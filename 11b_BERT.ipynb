{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "LFXSpzSnUGWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320f7664-c03a-4327-b30a-bdb789e5c084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "npU09sSAlytt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "q4hLoy30l3g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_for_bert = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "t9InjEayUexu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_question_answer(question, passage, max_len=500):\n",
        "    \n",
        "    \"\"\"\n",
        "    question: What is the name of YouTube Channel\n",
        "    passage: Watch complete playlist of Natural Language Processing. Don't forget to like, share and subscribe my channel CloudyML\n",
        "    \"\"\"\n",
        "\n",
        "    #Tokenize input question and passage \n",
        "    #Add special tokens - [CLS] and [SEP]\n",
        "    input_ids= tokenizer_for_bert.encode (question, passage, max_length= max_len, truncation=True)\n",
        "     \n",
        "    \"\"\"\n",
        "    [101, 2054, 2003, 1996, 2171, 1997, 7858, 3149, 102, 3422, 3143, 2377, 9863, 1997, 3019, 2653, 6364, 1012, \n",
        "    2123, 1005, 1056, 5293, 2000, 2066, 1010, 3745, 1998, 4942, 29234, 2026, 3149, 1045, 2290, 6627, 2136, 102]\n",
        "    We can see that 101 and 102 is [CLS] and [SEP] indicates that Starting and ending of sentence right?\n",
        "    \"\"\"\n",
        "\n",
        "    #Getting number of tokens in 1st sentence (question) and 2nd sentence (passage that contains answer)\n",
        "    sep_index = input_ids.index(102)\n",
        "    len_question = sep_index + 1\n",
        "    len_passage = len(input_ids) - len_question\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    8\n",
        "    9\n",
        "    27\n",
        "    \"\"\"\n",
        "    \n",
        "    #Need to separate question and passage\n",
        "    #Segment ids will be 0 for question and 1 for passage\n",
        "    segment_ids = [0]* len_question + [1] *(len_passage)\n",
        "    \"\"\"\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    #Converting token ids to tokens\n",
        "    tokens =tokenizer_for_bert.convert_ids_to_tokens (input_ids)\n",
        "    \"\"\"\n",
        "    tokens = ['[CLS]', 'what', 'is', 'the', 'name', 'of', 'youtube', 'channel', '[SEP]', 'watch', 'complete', \n",
        "    'play', '##list', 'of', 'natural', 'language', 'processing', '.', 'don', \"'\", 't', 'forget', 'to', 'like', \n",
        "    ',', 'share', 'and', 'sub', '##scribe', 'my', 'channel', 'i', '##g', 'CLoudyML, '[SEP]']\n",
        "    \"\"\"\n",
        "\n",
        "    #Getting start and end scores for answer\n",
        "    #Converting input arrays to torch tensors before passing to the model\n",
        "    start_token_scores = model(torch.tensor ([input_ids]), token_type_ids=torch.tensor ([segment_ids]) )[0] \n",
        "    end_token_scores = model(torch.tensor ([input_ids]), token_type_ids=torch.tensor ([segment_ids]) )[1]\n",
        "\n",
        "    \"\"\"\n",
        "    tensor([[-5.9787, -3.0541, -7.7166, -5.9291, -6.8790, -7.2380, -1.8289, -8.1006,\n",
        "         -5.9786, -3.9319, -5.6230, -4.1919, -7.2068, -6.7739, -2.3960, -5.9425,\n",
        "         -5.6828, -8.7007, -4.2650, -8.0987, -8.0837, -7.1799, -7.7863, -5.1605,\n",
        "         -8.2832, -5.1088, -8.1051, -5.3985, -6.7129, -1.4109, -3.2241,  1.5863,\n",
        "         -4.9714, -4.1138, -5.9107, -5.9786]], grad_fn=<SqueezeBackward1>)\n",
        "    tensor([[-2.1025, -2.9121, -5.9192, -6.7459, -6.4667, -5.6418, -1.4504, -3.1943,\n",
        "         -2.1024, -5.7470, -6.3381, -5.8520, -3.4871, -6.7667, -5.4711, -3.9885,\n",
        "         -1.2502, -4.0869, -6.4930, -6.3751, -6.1309, -6.9721, -7.5558, -6.4056,\n",
        "         -6.7456, -5.0527, -7.3854, -7.0440, -4.3720, -3.8936, -2.1085, -5.8211,\n",
        "         -2.0906, -2.2184,  1.4268, -2.1026]], grad_fn=<SqueezeBackward1>)\n",
        "    \"\"\"\n",
        "\n",
        "    #Converting scores tensors to numpy arrays\n",
        "    start_token_scores= start_token_scores.detach().numpy().flatten() \n",
        "    end_token_scores = end_token_scores.detach().numpy().flatten()\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    [-5.978666  -3.0541189 -7.7166095 -5.929051  -6.878973  -7.238004\n",
        "    -1.8289301 -8.10058   -5.9786286 -3.9319289 -5.6229596 -4.191908\n",
        "    -7.20684   -6.773916  -2.3959794 -5.942456  -5.6827617 -8.700695\n",
        "    -4.265001  -8.09874   -8.083673  -7.179875  -7.7863474 -5.16046\n",
        "    -8.283156  -5.108819  -8.1051235 -5.3984528 -6.7128663 -1.4108785\n",
        "    -3.2240815  1.5863497 -4.9714    -4.113782  -5.9107194 -5.9786243]\n",
        "\n",
        "    [-2.1025064 -2.912148  -5.9192414 -6.745929  -6.466673  -5.641759\n",
        "    -1.4504088 -3.1943028 -2.1024144 -5.747039  -6.3380575 -5.852047\n",
        "    -3.487066  -6.7667046 -5.471078  -3.9884708 -1.2501552 -4.0868535\n",
        "    -6.4929943 -6.375147  -6.130891  -6.972091  -7.5557766 -6.405638\n",
        "    -6.7455807 -5.0527067 -7.3854156 -7.043977  -4.37199   -3.8935976\n",
        "    -2.1084964 -5.8210607 -2.0906193 -2.2184045  1.4268283 -2.1025767]\n",
        "    \"\"\"\n",
        "    #Getting start and end index of answer based on highest scores\n",
        "    answer_start_index = np.argmax (start_token_scores)\n",
        "    answer_end_index =np.argmax(end_token_scores)\n",
        "\n",
        "    \"\"\"\n",
        "    31\n",
        "    34\n",
        "    \"\"\"\n",
        "\n",
        "    #Getting scores for start and end token of the answer\n",
        "    start_token_score =np.round(start_token_scores [answer_start_index], 2) \n",
        "    end_token_score = np.round(end_token_scores[answer_end_index], 2)\n",
        "\n",
        "    \"\"\"\n",
        "    1.59\n",
        "    1.43\n",
        "    \"\"\"\n",
        "\n",
        "    #Combining subwords starting with ## and get full words in output. \n",
        "    #It is because tokenizer breaks words which are not in its vocab.\n",
        "    answer=tokens [answer_start_index]\n",
        "    for i in range (answer_start_index + 1, answer_end_index + 1):\n",
        "      if tokens[i][0:2] == '##': \n",
        "        answer += tokens[i][2:]\n",
        "      else:\n",
        "        answer += tokens[i]\n",
        "\n",
        "    # If the answer didn't find in the passage\n",
        "\n",
        "    if ( answer_start_index == 0) or (start_token_score < 0) or (answer=='SEP') or ( answer_end_index < answer_start_index):\n",
        "      answer = \"Sorry!, I could not find an answer in the passage.\"\n",
        "\n",
        "    return (answer_start_index, answer_end_index, start_token_score, end_token_score, answer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # If the answer didn't find in the passage\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Testing function\n",
        "bert_question_answer(\"What is the name of YouTube Channel\", \" Don't forget to like, share and subscribe my channel CloudyML\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCM4qvUHl7cj",
        "outputId": "2d9f809c-81ea-43a8-9583-3374812c0e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22, 23, 6.49, 7.19, 'cloudyml')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "passage= \"\"\"NLP is a subfield of computer science and artificial intelligence concerned with interactions between \n",
        "computers and human (natural) languages. It is used to apply machine learning algorithms to text and speech. For \n",
        "example, we can use NLP to create systems like speech recognition, document summarization, machine translation, spam \n",
        "detection, named entity recognition, question answering, autocomplete, predictive typing and so on. Nowadays, most of \n",
        "us have smartphones that have speech recognition. These smartphones use NLP to understand what is said. Also, many \n",
        "people use laptops which operating system has a built-in speech recognition. NLTK (Natural Language Toolkit) is a \n",
        "leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces \n",
        "to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, \n",
        "tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, \n",
        "community-driven project. We’ll use this toolkit to show some basics of the natural language processing field. For \n",
        "the examples below, I’ll assume that we have imported the NLTK toolkit. We can do this like this: import nltk. \n",
        "Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into \n",
        "its component sentences. The idea here looks very simple. Word tokenization (also called word segmentation)\n",
        "is the problem of dividing a string of written language into its component words. In English and many other languages\n",
        "using some form of Latin alphabet, space is a good approximation of a word divider. However, we still can have problems\n",
        "we only split by space to achieve the wanted results. Some English compound nouns are variably written and sometimes\n",
        "they contain a space. In most cases, we use a library to achieve the wanted results, so again don’t worry too much \n",
        "for the details. Stop words are words which are filtered out before or after processing of text. When applying machine\n",
        "learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words.\n",
        "Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single\n",
        "universal list of stopwords. The list of the stop words can change depending on your application. The NLTK tool has\n",
        "a predefined list of stopwords that refers to the most common words. If you use it for your first time, you need to\n",
        "download the stop words using this code: nltk.download(“stopwords”). Once we complete the downloading, we can load\n",
        "the stopwords package from the nltk.corpus and use it to load the stop words.\"\"\"\n",
        "\n",
        "print (f'Length of the passage: {len (passage.split())} words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfJV2ue7mHLA",
        "outputId": "a5cbdfff-beaa-4c4c-a194-5a4fc55f4c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the passage: 433 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "SoRv6vvig46W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"What is full form of NLTK\"\n",
        "print ('\\nQuestion 1:\\n', question)\n",
        "answer_start_index, answer_end_index, start_token_score, end_token_score, ans =bert_question_answer( question, passage)\n",
        "print(\"\\nBERT ANSWER:\" , ans , '\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05JNTXbmgdVo",
        "outputId": "3d74ce63-fbbc-497e-bf0f-2d4a33d2aced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 1:\n",
            " What is full form of NLTK\n",
            "\n",
            "BERT ANSWER: naturallanguagetoolkit \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"What are stop words \"\n",
        "print ('\\nQuestion 2:\\n', question)\n",
        "answer_start_index, answer_end_index, start_token_score, end_token_score, ans =bert_question_answer( question, passage)\n",
        "print(\"\\nBERT ANSWER: \" , ans , '\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCH_fyxCihWR",
        "outputId": "00599731-9ae2-4a0f-e694-15823fe32935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 2:\n",
            " What are stop words \n",
            "\n",
            "BERT ANSWER:  wordswhicharefilteredoutbeforeorafterprocessingoftext \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"What is NLP \"\n",
        "print ('\\nQuestion 3:\\n', question)\n",
        "answer_start_index, answer_end_index, start_token_score, end_token_score, ans =bert_question_answer( question, passage)\n",
        "print(\"\\nBERT ANSWER: \" , ans , '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5zfA4biikK3",
        "outputId": "0ef73cd8-b33a-47df-9e63-3d0ae252e91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 3:\n",
            " What is NLP \n",
            "\n",
            "BERT ANSWER:  asubfieldofcomputerscienceandartificialintelligenceconcernedwithinteractionsbetweencomputersandhuman(natural)languages \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"How to download stop words from nltk\"\n",
        "print ('\\nQuestion 4:\\n', question)\n",
        "answer_start_index, answer_end_index, start_token_score, end_token_score, ans =bert_question_answer( question, passage)\n",
        "print(\"\\nBERT ANSWER: \" , ans , '\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OIq3CxYiqZE",
        "outputId": "0ea3e490-3c9f-4122-fadf-7ecfc29def46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 4:\n",
            " How to download stop words from nltk\n",
            "\n",
            "BERT ANSWER:  importnltk \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"What is supervised learning\"\n",
        "print ('\\nQuestion 6:\\n', question)\n",
        "answer_start_index, answer_end_index, start_token_score, end_token_score, ans =bert_question_answer( question, passage)\n",
        "print(\"\\nBERT ANSWER: \" , ans , '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVsdDgrBiy9p",
        "outputId": "47fda9ee-b246-46fc-c00f-d584c4c423d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 6:\n",
            " What is supervised learning\n",
            "\n",
            "BERT ANSWER:  Sorry!, I could not find an answer in the passage. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Question-Answering Application { vertical-output: true }\n",
        "#@markdown ---\n",
        "#define question\n",
        "question= \"How to download stop words from nltk?\" #@param {type:\"string\"}\n",
        "#define passage\n",
        "passage = \"\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#apply bert_question_answer\n",
        "answer_start_index, answer_end_index, start_token_score, end_token_score,ans  = bert_question_answer( question, passage)\n",
        "#print the answer\n",
        "#@markdown Answer:\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t8wF2enmLX5",
        "outputId": "731d7370-e165-42dc-f83b-9a3f17a35d8c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry!, I could not find an answer in the passage.\n"
          ]
        }
      ]
    }
  ]
}